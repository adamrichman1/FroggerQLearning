# CS1571 Project 3 (Fall 2018)

CS 1571 - Project 3: Q-Learning with Frogger

## Author
* [Adam Richman](https://www.github.com/adamrichman1)

## Project URL
- https://github.com/PittCS1571/adr60-project3

## Configuration File
The configuration file, *q_values.json*, contains a Python dictionary dumped to JSON. The python dictionary maps 
state IDs to arrays. The first array element contains a Python dictionary mapping that state's actions to q-values.
The second array element contains a Python dictionary mapping that state's actions to N, the number of times the 
state-action pair has been seen.

This data file is written to each iteration of the program, so make sure to run:
<code>
git reset --hard HEAD
</code>
to run the submitted version.

## Run with Q-Learning Configuration file
<code>
git reset --hard HEAD

python Qagent.py 0
</code>

## Run WITHOUT Q-Learning Configuration file (train)
<code>
python Qagent.py 1
</code>

## Final Agent Performance
My agent, on average, reaches 1 home per round. That is, on average, he will successfully make it home
one time, die, and then reach home again. Occasionally the frog will reach 2 homes in one round,
and I have seen 3 on rare occasions. The agent will usually fill either the rightmost home or the one 
directly to the left, but in past I have seen the frog reach homes to the left.

My frog trained for roughly an hour before submission. I was able to have success so quickly because I did not
have too large of a state space.

Once my agent is trained, he records, on average, a reward of 0.16 per round (per death).

## State Space
Each state can be identified by a stringified series of integers (e.g. 151111111):

1. The distance in block sized-chunks (32 pixels) that the frog is from the row of homes (3-15).

2. An indicator variable representing whether the space directly to the right is safe (0-1).

3. An indicator variable representing whether the space directly ahead is safe (0-1).

4. An indicator variable representing whether the space directly behind is safe (0-1).

5. An indicator variable representing whether the current space is safe (0-1).

6. An indicator variable representing whether the space directly in front is an empty home (0-1).

7. An indicator variable representing whether the frog is against the left wall (0-1).

8. An indicator variable representing whether the frog is against the right wall (0-1).

The Python dictionary *q_table* in class QAgent maps state IDs to a state's Q-Values and N-Values. If the state s' that 
is generated by a state-action pair (e.g. s' = arg_max(s, a)) has an ID that has already been seen before, 
I update that state's Q & N values to reflect what has already been saved in *q_table*. I then increment the respective
Q & N values. This allows my agent to recognize states in one area of the board as the same as another, since ID is mapped
to the previous Q & N values.

To simplify the game, I noticed a pattern between the Cars and Rivers sections. In the Cars section, all objects are
obstacles and all empty space is safe. In the Rivers section, all objects are safe and all empty space are obstacles.
Thus, when I create my states, I simply flip the bit as opposed to the other section (0 or 1) depending on which section 
of the game the frog is in. By doing this, the frog was able to learn how to solve the Rivers section by solving the
Cars section.

I tried a variety of other state representations. At one point, I considered the frog's X position in the state as well.
However, I learned that including X position increases learning time significantly Additionally, when run overnight, the frog
did not see much improvement compared to now. Other variables included distance from cars and distance from start, and I
removed them for similar reasons.

## Parameters

My game utilizes 3 parameters that drive learning: *alpha*, *gamma*, an an *inflation constant*. 

- *Alpha* is set to (2/n), where n is the number of times the specific state-action pair (s, a) has been chosen. 
It acts as each state's unique learning rate. Lecture 14 recommends using a value of (1/n), but considering the 
relatively small state space of my agent, I decided to use (2/n) so that each state must be visited more often 
before its q-value converges.

- *Gamma* is set at a constant 0.2. This means that when updating the Q-value, the immediate reward will contribute 80%
and the future reward contributes 20%.

- The *Inflation Constant* is set at a constant value of 2. This encourages exploration of state-action pairs that 
otherwise would have been too unappealing to consider. For example, suppose my agent travels to s' from (s, a) and then
dies in s'. Normally, (s, a) would have a very low value. The inflation constant raises it slightly so it is still
considered next time the agent reaches the state. I originally had this value set at 4, but noticed the frog getting
into infinite loops because death states were being considered too highly.

## Modified Reward

I slightly modified the reward that the game had already implemented. I DO NOT offer a positive reward for moving upward,
nor do I offer a negative reward for moving downward. Some changes I made include...

- Changing victory reward to 3.0 from 1.0
- Small positive reward for reaching intersection
- Small negative reward if a cycle has been detected (the frog returned to a previous state forming a cycle of size
*(3 < x < 6)* iterations
- Small negative reward for backtracking past the intersection
- Small negative reward for aimlessly wandering around bottom two rows or the intersection

## Improvement

I noticed close to submission that I forgot to include whether or not the space directly to the left was safe. I had
coded the variable, but did not realize I had forgotten it in the state ID. Including this would surely improve
my agent's performance. Furthermore, and as mentioned earlier, including the frog's x position would have also 
improved performance.

## Collaborators

I talked to Riley Marzka about this project. We discussed strategy in terms of approaching different aspects of the game.
This includes how the rivers section should be handled as opposed to the cars section. We also discussed issues we had
during development, such as why a frog might be stuck in an infinite loop.

